# Loss Function
# Date : 6/3/2018

Loss Function :

Softmax Function is a activation function given by the following formula 

softmax function =  (e^wxi) / sigma j = 1 to k (e^wxj)

Now this function allows us to interpret the output as probabilities since it is normalized. Now the Cost entropy error  is the measure of 
error  calculated at this softmax layer given by 

    softmax function              −log(efyi∑jefj)

# More on softmax 

The softmax maps arbitary value of xi to a probability distribution pi. It is called softmax because , 

"MAX" - The Numerator (probabilty ) is maxmized values for larger value of xi. 
"Soft" - It still assign some minimal probability for low value xi as well (Not so rigid)

Intution of how softmax assign probability to the dot product of w.x is easy to intrepet from above conclusion . If w and x are similiar 
then the dot product is higher and thus higher probability.


# Spiritual Lesson - 13/3/2018

Death and life are in the power of the tongue, and those who love it will eat its fruits. - Proverb 18:21.

Yes looks like a known verse, but I as a normal human often blaber with my mouth about my current state of affair and future state of affair.
Even when I have big dreams and have faith and pray about it I some times utter some non sensical word about my future.
One classic Example : Even we are placed in a wonderful project , good respect and great onsite oppurtunity and desire to continue in the project,
we complain with my mouth that timing is an issue and if only I am in project with great timing and nearby location that would be great.
Ha ha when that happens and I get a project with the desired timing and location and every thing else is in pathetic condition you realise 
oh  God has placed us in a wonderful situation but I have wasted it with my own mouth.

Lesson : Be careful with what you speak from your mouth.

Lesson 2: Instead Please say let the will of God be accomplished in my life as Jesus said, because the Bible says - Isaiah 55:8

"For My thoughts are not your thoughts, Nor are your ways My ways," declares the LORD. 9"For as the heavens are higher than the earth, So are My ways higher than your ways And 
My thoughts than your thoughts"

Seek His guidance and will in every new step you take and obey it even if it difficult

# Lessson 3 : God is strict in money matters , even if it one rupees , He does ensure it is collected if we have owed Him. 
Delay is also not accepted

# Some lesson for personal development

1. Don't try to complete all work in a single sitting. Try splitting the work and complete it. Ideal sitting time 1.5 hrs 

2. Don't postpone the work thinking you would do it perfectly after getting more input. It is better to have imperfect work than to 
have nothing in hand. So the lesson is # Dont postpone , if it is small work complete it immediatly (sending the mail etc..)

# Some God given Links

# Dashboard Link

http://www.myhss.org/downloads/board/regular_meetings/2016/RM_011416_HealthPlansDashboardQ2_2015.pdf  

#Dataset removed from Web

https://foreverdata.org/datasets.html

Resources
http://www.ritchieng.com/machine-learning-resources/

class Titanic_Embedding_Neural(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size):
        super(Titanic_Embedding_Neural, self).__init__()
        self.Pclassembeddings = nn.Embedding(input_size, embedding_dim)
		self.Embarkedembeddings = nn.Embedding(Barkinput_size, Barkembedding_dim)
		self.Personembeddings = nn.Embedding(Personinput_size, Personembedding_dim)
        self.linear1 = nn.Linear(embedding_dim+Barkembedding_dim+Personembedding_dim+len(cont_variable)], 128)
		self.relu = nn.ReLU()
		self.linear2 = nn.Linear(128, 2)

    def forward(self, input):
        e1 = self.embeddings(input[Pclass].aslist).view((-1, Pclassembeddings.dim))  # pass the input Pclass as list for all passengers  and reshape to the dimension of embedding vector
        e2 = self.Embarkedembeddings(input[Embarked].aslist).view((-1, Barkembeddings_dim))
		e3 = self.Personembeddings(input[Person].aslist).view((-1, Personembeddings_dim))	
		c1 = input['cont_variable']
        x = torch.cat((e1, e2, e3,c1), 1) # need to try and see if you are getting the correct dimensions
		out = self.linear1(x)
		out = self.relu(out)
		out = self.linear2(out)
        return out
		
		
https://github.com/entron/entity-embedding-rossmann/blob/kaggle/models.py

Use for inspiration

# Pytorch 

Differential learning rate
https://discuss.pytorch.org/t/how-to-perform-finetuning-in-pytorch/419/7



